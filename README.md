# ğŸ¥ Audio Transcription & Sentiment Analysis Pipeline

This project consists of two Python programs:

1. **Part 1** - Extracts audio from video files, performs speech-to-text transcription, and classifies sentiment for each spoken segment.
2. **Part 2** - Visualizes the transcription data using meaningful plots.

---

## ğŸ“ Part 1: Transcription & Sentiment Extraction

This notebook processes all video files placed in the `input_videos/` directory:

-  Extracts audio using `moviepy`
-  Splits audio into 5-second chunks using `pydub`
-  Transcribes speech using OpenAI's `whisper` model
-  Analyzes sentiment using `transformers` (`distilbert-base-uncased-finetuned-sst-2-english`)
-  Outputs a `.csv` with `timestamp`, `transcription`, and `sentiment` columns in `output_data/`

### To Run:
```bash
python Part1_Transcription_Pipeline.ipynb
```

---

## ğŸ“Š Part 2: Visualization of Transcription Data

This notebook takes the `.csv` generated by Part 1 and displays:

1.  Histogram of **word count per 5-second bucket**
2.  Bar chart of **sentiment distribution**
3.  Line plot of **sentiment trend over time**

### To Run:
Open the notebook:
```bash
Part2_Visualization.ipynb
```
Replace `"your_csv_file.csv"` with your actual output file from `output_data/`.

---

## ğŸ“‚ Folder Structure

```
project/
â”œâ”€â”€ Part1_Transcription_Pipeline.ipynb
â”œâ”€â”€ Part2_Visualization.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## How It Was Tested

- Tested with short `.mp4` video files containing spoken English
- Whisper used in CPU fallback mode (`FP32`) if GPU unavailable
- Output validated via CSV inspection and visualizations

---

##  Requirements

Install all dependencies with:

```bash
pip install -r requirements.txt
```

### Libraries Used:
- `pandas`
- `moviepy`
- `pydub`
- `whisper`
- `transformers`
- `torch`
- `matplotlib`
- `seaborn`

---

## ğŸ’¡ Potential Extensions

- Speaker diarization
- Real-time inference with streaming input
- Interactive Streamlit dashboard


